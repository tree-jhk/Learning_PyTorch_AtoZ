{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "신경망 생성 및 학습을 돕기 위해 PyTorch에서 제공하는 도구들:\n",
    "torch.nn, torch.optim, Dataset 그리고 DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import requests\n",
    "\n",
    "DATA_PATH = Path(\"data\")\n",
    "PATH = DATA_PATH / \"mnist\"\n",
    "\n",
    "PATH.mkdir(parents=True,exist_ok=True)\n",
    "\n",
    "URL = \"https://github.com/pytorch/tutorials/raw/master/_static/\"\n",
    "FILENAME = \"mnist.pkl.gz\"\n",
    "\n",
    "if not (PATH / FILENAME).exists():\n",
    "        content = requests.get(URL + FILENAME).content\n",
    "        (PATH / FILENAME).open(\"wb\").write(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import gzip\n",
    "\n",
    "with gzip.open((PATH / FILENAME).as_posix(), \"rb\") as f:\n",
    "    ((x_train,y_train),(x_valid,y_valid),_) = pickle.load(f, encoding=\"latin-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 784)\n",
      "<class 'numpy.ndarray'>\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot\n",
    "import numpy as np\n",
    "\n",
    "pyplot.imshow(x_train[2].reshape((28,28)),cmap=\"gray\")\n",
    "print(x_train.shape)\n",
    "print(type(x_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]]) tensor([5, 0, 4,  ..., 8, 4, 8])\n",
      "torch.Size([50000, 784])\n",
      "tensor(0.) tensor(9)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# map을 통해 나눠놔던 (x_train,y_train,x_valid,y_valid)을 tensor로 변환함.\n",
    "x_train,y_train,x_valid,y_valid = map(\n",
    "    torch.tensor,(x_train,y_train,x_valid,y_valid)\n",
    ")\n",
    "n,c = x_train.shape\n",
    "print(x_train,y_train)\n",
    "print(x_train.shape)\n",
    "print(x_train.min(),y_train.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch는 랜덤, 0, 1로만 이루어진 tensor를 생성하는 메서드를 제공한다.\n",
    "우리는 간단한 선형 모델의 가중치(weights)와 절편(bias)를 생성하기 위해 이것들을 사용할 것이다.\n",
    "\n",
    "[ 랜덤, 0, 1로만 이루어진 tensor ] 의 역할: PyTorch에게 이들이 기울기(gradient)가 필요하다고 알려준다.\n",
    "이를 통해, PyTorch는 tensor에 행해지는 모든 연산을 기록하게 하고,\n",
    "-> 행해진 모든 연산을 기록했기 때문에, 자동적으로 back-propagation이 진행되면서 기울기를 계산한다.\n",
    "\n",
    "가중치는 requires_grad를 초기화(initialization) 다음에 설정한다.\n",
    "\n",
    "(in-place) 연산 뜻: 바꿔치기 연산이라는 뜻이다.\n",
    "추가적인 메모리가 들지 않는다.\n",
    "a.fill_(3.5)\n",
    "a의 값이 3.5로 바꿔치기 됩니다. 이런 느낌.\n",
    "\n",
    "PyTorch에서 _ 다음에 오는 메서드 이름은 연산이 in-place로 수행되는 것을 의미한다.\n",
    "weights.requires_grad_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "tensor([[3.5000, 3.5000, 3.5000],\n",
      "        [3.5000, 3.5000, 3.5000],\n",
      "        [3.5000, 3.5000, 3.5000]])\n",
      "tensor([[4.5000, 4.5000, 4.5000],\n",
      "        [4.5000, 4.5000, 4.5000],\n",
      "        [4.5000, 4.5000, 4.5000]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.zeros(3,3)\n",
    "b = torch.ones(3,3)\n",
    "print(a)\n",
    "print(a+b)\n",
    "a.fill_(3.5)\n",
    "print(a)\n",
    "print(a+b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.0413, -1.2746, -1.1292,  ..., -0.4048,  0.4602, -0.2642],\n",
      "        [ 0.5860,  0.9938, -0.7320,  ...,  1.4435,  0.1860, -0.1217],\n",
      "        [-0.1196,  0.5060,  1.1930,  ..., -0.7262,  1.2742, -0.2020],\n",
      "        ...,\n",
      "        [-0.4376,  0.0221,  1.5616,  ...,  0.6013, -0.1565,  1.5370],\n",
      "        [ 2.2324,  0.4808, -0.2327,  ...,  1.4464, -0.5967,  0.6577],\n",
      "        [-1.4010,  1.0267, -1.1726,  ..., -0.0085,  1.9846,  0.0880]])\n",
      "tensor([[ 0.0372, -0.0455, -0.0403,  ..., -0.0145,  0.0164, -0.0094],\n",
      "        [ 0.0209,  0.0355, -0.0261,  ...,  0.0516,  0.0066, -0.0043],\n",
      "        [-0.0043,  0.0181,  0.0426,  ..., -0.0259,  0.0455, -0.0072],\n",
      "        ...,\n",
      "        [-0.0156,  0.0008,  0.0558,  ...,  0.0215, -0.0056,  0.0549],\n",
      "        [ 0.0797,  0.0172, -0.0083,  ...,  0.0517, -0.0213,  0.0235],\n",
      "        [-0.0500,  0.0367, -0.0419,  ..., -0.0003,  0.0709,  0.0031]])\n"
     ]
    }
   ],
   "source": [
    "# weights = (torch.randn(784, 10,requires_grad=True) / math.sqrt(784))\n",
    "\n",
    "import math\n",
    "\n",
    "# Xavier initialisation 기법 사용\n",
    "# (1/sqrt(n)을 곱해주는 것을 통해서 초기화)\n",
    "\n",
    "# 가중치가 random임\n",
    "weights = torch.randn(784,10)\n",
    "print(weights)\n",
    "weights = weights / math.sqrt(784)\n",
    "print(weights)\n",
    "\n",
    "# 위의 초기화 단계 (randn)가 기울기에 포함되지 않기 위해 초기화 후\n",
    "# requires_grad를 설정해준다.\n",
    "weights.requires_grad_()\n",
    "\n",
    "# bias 주목할 점이, requires_grad=True 라는 점이다.\n",
    "# 계속해서 bias를 포함해서 계산하거나, bias를 포함해서 계산했던 값들에\n",
    "# 연산이 계속 기록된다.(PyTorch의 고유한 기능)\n",
    "bias = torch.zeros(10, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_softmax(x):\n",
    "    return x - x.exp().sum(-1).log().unsqueeze(-1)\n",
    "\n",
    "# @: 행렬 곱셈(matrix multiplication) 연산을 의미한다.\n",
    "# model(입력값 x)를 통해서 return 되는 값:\n",
    "# (x*w + b)의 softmax 활성화 함수를 통과해서 나온 결과\n",
    "# 이제는 다양한 비선형 데이터를 처리하는 함수들이 많지만, 여기서는 선형 방정식인\n",
    "# y = x*w + b를 사용한다.\n",
    "\n",
    "# 처음에 가중치 w를 랜덤으로 추출했기 때문에, 맨 처음에 model()을 통과한 결과는\n",
    "# 그냥 랜덤으로 값을 예측한 것에 비해 나은 점이 없다.\n",
    "\n",
    "# model함수는 bais를 사용한다.\n",
    "def model(xb):\n",
    "    return log_softmax(xb @ weights + bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-2.2587, -2.4193, -2.4787, -2.0466, -2.1802, -2.1293, -2.2182, -2.6956,\n",
      "        -2.2400, -2.5409], grad_fn=<SelectBackward>) torch.Size([64, 10])\n"
     ]
    }
   ],
   "source": [
    "# forward pass 과정\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "# 처음에 가중치 w를 랜덤으로 추출했기 때문에, 맨 처음에 model()을 통과한 결과는\n",
    "# 그냥 랜덤으로 값을 예측한 것에 비해 나은 점이 없다.\n",
    "xb = x_train[0:batch_size]\n",
    "\n",
    "# preds는 bias를 사용하는 model함수를 사용한다.\n",
    "preds = model(xb)\n",
    "preds[0], preds.shape\n",
    "print(preds[0], preds.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "preds 텐서는 tensor 값 외에도, grad_fn=<SelectBackward> 라는 gradient function을 담고 있다.\n",
    "\n",
    "이 gradient function인 grad_fn=<SelectBackward>은 나중에 역전파(backpropagation)과정을 위해 사용한다.\n",
    "-> 나중에 역전파 과정으로 기울기값을 구하기 위해 사용할 것이다라는 말."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 손실함수(loss function)을 사용하기 위한 음의 로그 우도 구현\n",
    "# 음의 로그 우도(negative log-likelihood):\n",
    "# 다중 클래스 분류에서 자주 사용되는 목적 함수이다.\n",
    "# 직관적인 이해:\n",
    "#   모델이 자신의 대답에 완전히 확신하고 대답이 잘못되면 손실이 높아진다.\n",
    "#   모델이 자신의 대답에 완전히 확신하고 대답이 맞게 되면 손실이 낮아진다.\n",
    "\n",
    "# 목적 함수(손실함수, 비용 함수, loss function, cost function)은 \n",
    "# 네트워크가 최소화되도록 학습되는 함수이다.\n",
    "\n",
    "# input: 예측 결과\n",
    "# target: 정답 데이터\n",
    "def negative_log_likelihood(input,target):\n",
    "    return -input[range(target.shape[0]),target].mean()\n",
    "\n",
    "loss_func = negative_log_likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.2899, grad_fn=<NegBackward>)\n"
     ]
    }
   ],
   "source": [
    "# yb는 정답 데이터(label 데이터)\n",
    "yb = y_train[0:batch_size]\n",
    "\n",
    "# 위에서 정의한 negative_log_likelihood를 통과한 결과로 loss 계산\n",
    "# loss_func은 bias를 사용하는 model함수를 사용하는 preds를 사용한다.\n",
    "# 따라서 grad_fn이 같이 사용된다.\n",
    "print(loss_func(preds,yb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정확도(accuracy)를 계산하기 위한 함수 구현\n",
    "# 매 예측마다, 만약 가장 큰 값의 인덱스가 목표값과 동일하다면 그 예측은 올바른 것\n",
    "\n",
    "# output: 모델을 통해 계산된 결과값\n",
    "# yb는 정답 데이터(label 데이터)\n",
    "def accuracy(output,yb):\n",
    "    preds = torch.argmax(output,dim=1)\n",
    "    return (preds == yb).float().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정확도: 6.25%\n"
     ]
    }
   ],
   "source": [
    "print(\"정확도: \",float(accuracy(preds, yb))*100,\"%\",sep=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "저 위에까지가, 1 epoch의 train 후 정확도를 구한 과정이다.\n",
    "\n",
    "이제는 loop를 통해서 여러 epoch를 돌려서 정확도를 개선해보자."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "앞으로 수행할 것들:\n",
    "1. 데이터의 미치배치를 선택(batch_size 크기 지정)\n",
    "2. 모델을 이용하여 예측 수행\n",
    "3. 손실 계산\n",
    "4. loss.backward()를 이용하여 모델의 기울기 업데이트, 이 경우에는 weights와 bias를 업데이트.\n",
    "\n",
    "가중치와 절편 업데이트는 torch.no_grad()라는 context manager 내에서 실행한다.\n",
    "이유: 가중치와 절편 업데이트가 다음 기울기의 계산에 기록되지 않기를 원하기 때문이다.\n",
    "\n",
    "Context Manager란?:\n",
    "1. \"with문을 실행할 때 설정하는 런타임 컨텍스트를 정의하는 객체\" 라고 한다.\n",
    "2. Context Manager는 with문을 사용하여 호출되고, 원하는 타이밍에 정확하게 리소스를 할당하고 제공하는 역할을 한다.\n",
    "3. 즉, 특정 행동을 할 때 항상 일정한 런타임 환경을 만들어주기 위해 특정 행동에의 진입과 종료를 관리해주는 친구가 바로 context manager이라는 것이다.\n",
    "\n",
    "한마디로, with문 내에 어떤 특정한 환경을 만들어서 뭔가 따로 실행하게 하는 것이다.\n",
    "\n",
    "torch.no_grad()란?:\n",
    "gradient 연산 옵션을 끌 때 사용하는 파이썬 컨텍스트 매니저이다.\n",
    "즉, with torch.no_grad(): 을 하지 않고 계산하면, 계산 결과가 계속 기록될(auto grad가 자동 기록) 것이다.\n",
    "\n",
    "Autograd: .grad 속성에 있는 각 모델 파라미터에 대한 기울기를 계산하고 저장한다.\n",
    "나중에 .backward()를 호출하면 autograd가 기울기를 계산해준다.\n",
    ".backward()는 Require_grad=True로 설정된 모든 텐서들에 대해 gradient를 계산한다.\n",
    "\n",
    "backpropagation에서 gradient를 계산하는 첫 starting point가 되는 값은 loss 값이다.\n",
    "우리는 이 loss값을 모든 가중치들(parameters; weight and bias)에 대해서(with respect to) 미분을 계산하게 된다.\n",
    "\n",
    "이러한 .backward() 메서드는 gradient 계산이 시작되는 지점인 loss 변수에 적용해줘야 한다.\n",
    "\n",
    "loss.backward()를 제어하는 역할을 하는 변수는 bias, weights이다.\n",
    "왜냐면, 이들은 선언됐을 때부터 requires_grad = True로 설정했기 때문이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model() 함수와 loss 함수를 사용 후, 연산에 대한 기울기들을 이용해서\n",
    "가중치와 절편을 업데이트한다.\n",
    "\n",
    "업데이트를 하고나서 다시 기울기를 0으로 설정해서 다음 loop를 준비한다.\n",
    "0으로 초기화하지 않으면, 일어난 모든 연산의 누적 집계를 기록하게 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "lr = 0.5\n",
    "epochs = 2\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(2)\n",
    "    for i in range((n-1) // batch_size + 1):\n",
    "        start_i = i * batch_size\n",
    "        end_i = start_i + batch_size\n",
    "        xb = x_train[start_i:end_i]\n",
    "        yb = y_train[start_i:end_i]\n",
    "        pred = model(xb)\n",
    "        loss = loss_func(pred,yb)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            weights -= weights.grad * lr\n",
    "            bias -= bias.grad * lr\n",
    "            weights.grad.zero_()\n",
    "            bias.grad.zero_()\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "여기 위에까지가 아주아주 기본적인 신경망을 구현한 것이다.\n",
    "바로 단일층 신경망(single layer neural network)이다.\n",
    "입력층, 출력층, 활성화 함수가 있는 신경망이다.\n",
    "'은닉층'이 없다. 오늘날의 일반적인 신경망들은 은닉층(hidden layer)를 포함한다.\n",
    "은닉층이 많을수록(깊을수록) 비선형 데이터에 대해서 더 정확한 분류가 가능하지만,\n",
    "은닉층이 많을수록(깊을수록) train 데이터셋에 과적합(overfitting)된다.\n",
    "이러한 문제를 해결한 모델이 ResNet\n",
    "\n",
    "    참고:\n",
    "        로지스틱 회귀(다중 회귀) == 은닉층이 없는 신경망"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0829, grad_fn=<NegBackward>) tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "print(loss_func(model(xb), yb), accuracy(model(xb), yb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "코드 refactoring: 겉으로 보이는 동작의 변화 없이 소프트웨어 구조를 바꾼다는 뜻\n",
    "소프트웨어를 보다 쉽게 이해할 수 있고, 적은 비용으로 수정할 수 있도록 겉으로 보이는\n",
    "동작의 변화 없이 내부 구조를 변경하는 것을 말한다.\n",
    "코드 가독성과 질을 높이기 위한 목적으로 정리를 하는 것으로 생각할 수 있다.\n",
    "\n",
    "위에까지의 과정들을 PyTorch의 nn 클래스의 장점을 활용하여 더 간결하고 유연하게 만들어 보자.\n",
    "1. 작성했던 활성화 함수, 손실 함수 -> torch.nn.functional의 함수로 대체\n",
    "(관례에 따라, 일반적으로 torch.nn.functional을 F라는 네임스페이스(namespace)를 통해 임포트(import) 한다)\n",
    "torch.nn.functional 모듈에는 torch.nn 라이브러리의 모든 함수가 포함되어 있다.\n",
    "torch.nn.functional 에는 이외에도 풀링ㅎ 함수, 컨볼루션 연산, 선형 레이어 등이 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "# loss_func refactoring 전:\n",
    "\"\"\"\n",
    "def negative_log_likelihood(input,target):\n",
    "    return -input[range(target.shape[0]),target].mean()\n",
    "\n",
    "loss_func = negative_log_likelihood\n",
    "\"\"\"\n",
    "# loss_func refactoring 후:\n",
    "loss_func = F.cross_entropy\n",
    "\n",
    "def model(xb):\n",
    "    return xb @ weights + bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0829, grad_fn=<NllLossBackward>) tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "print(loss_func(model(xb), yb), accuracy(model(xb), yb))\n",
    "\n",
    "# 결과값이 같다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "명확하고 간결한 train loop를 위해\n",
    "nn.Module과 nn.Paramete를 사용한다.\n",
    "\n",
    "nn.Module:\n",
    "PyTorch에서 신경망은 보통 신경망 모듈(torch.nn.Module)을 '상속'받는 파이썬 클래스로 '정의'한다. -> 그냥 약속임. 그냥 신경망 클래스 정의할 때,\n",
    "class GNNStack(nn.Module): \n",
    "이런 식으로 쓰는 것이 약속되어 있다.(뇌피셜)\n",
    "\n",
    "nn.Module을 상속받으면 파이토치 프레임워크에 있는 각종 도구를 쉽게 적용할 수 있다.\n",
    "nn.Module (대문자 M) 은 PyTorch 의 특정 '개념'이고, 우리는 이 클래스를 많이 사용할 것입니다.\n",
    "\n",
    "nn.Module은 사용할 몇 가지 속성(attribute)와 메소드(.parameters() 와 .zero_grad() 같은) 가지고 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "# 위에까지에서는 model()함수, 초기화 등을 그냥 막 흩어져서 했는데,\n",
    "# 이제는 깔끔하게 하나의 클래스에 담아줘서 더 보기좋고 유연하게 됐다. (refactoring)\n",
    "\n",
    "# Refactoring 전(Mnist 데이터셋에 대한 다중 회귀를 위해 처음에 했던 코드):\n",
    "\"\"\"\n",
    "import math\n",
    "\n",
    "weights = torch.randn(784,10)\n",
    "weights = weights / math.sqrt(784)\n",
    "weights.requires_grad_()\n",
    "bias = torch.zeros(10, requires_grad=True)\n",
    "\n",
    "def model(xb):\n",
    "    return log_softmax(xb @ weights + bias)\n",
    "\"\"\"\n",
    "\n",
    "# Refactoring 후:\n",
    "\n",
    "class Mnist_Logistic(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.weights = nn.Parameter(torch.randn(784, 10) / math.sqrt(784))\n",
    "        self.bias = nn.Parameter(torch.zeros(10))\n",
    "\n",
    "    def forward(self, xb):\n",
    "        return xb @ self.weights + self.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이제 model() 함수를 사용하려면, class로 만들어진 모델 틀을 인스턴스화 한다.\n",
    "model = Mnist_Logistic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.5475, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "# PyTorch는 forward 메소드를 자동으로 호출한다.\n",
    "# nn.Modeul object들은 마치 함수처럼 사용된다.\n",
    "print(loss_func(model(xb), yb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nn.Module.parameters:\n",
    "모듈의 파라미터들을 iterator로 반환한다.\n",
    "nn.Module.parameters() 는 먼저 모든 하위 모듈들을 탐색하고(recursive=True), 각 모듈의 _parameters에 들어있는 파라메터들을 하나씩 반환해주는 함수이다.\n",
    "+---모듈 A\n",
    "|   +---모듈 B\n",
    "|   +---모듈 B\n",
    "|       +---모듈 D\n",
    "-> A.parameters()를 호출하면, [A.B.weight, A.B.bias, A.C.D.weight, A.C.D.bias]가 반환된다.\n",
    "결국, 일일히 weights, bias를 개별적으로 수동으로 0으로 제거할 필요가 없어진다.\n",
    "\n",
    "아래는 refactoring 전:\n",
    "with torch.no_grad():\n",
    "    weights -= weights.grad * lr\n",
    "    bias -= bias.grad * lr\n",
    "    weights.grad.zero_()\n",
    "    bias.grad.zero_()\n",
    "\n",
    "아래는 refactoring 후:\n",
    "with torch.no_grad():\n",
    "    for p in model.parameters():\n",
    "        p -= (p.grad() * lr)\n",
    "    model.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    for _ in range(epochs):\n",
    "        for i in range((n-1)//batch_size+1):\n",
    "            # dataset 선택을 batch_size와 i의 인덱스로 슬라이싱해서 선택함.\n",
    "            # 순서가 랜덤이 아니고 순서가 계속 유지되는 상황이다.\n",
    "            start_i = i * batch_size\n",
    "            end_i = start_i + batch_size\n",
    "            xb = x_train[start_i:end_i]\n",
    "            yb = y_train[start_i:end_i]\n",
    "            \n",
    "            # train 데이터의 feature 부분을 model()을 통해서 model() 내부에서\n",
    "            # 정의한 forward()로 계산한 결과를 pred에 저장함.\n",
    "            pred = model(xb)\n",
    "            # 위에서 정의한 loss_func으로 예측한 결과인 pred와\n",
    "            # 정답 데이터인 yb 사이의 loss를 계산해서 loss에 저장함.\n",
    "            loss = loss_func(pred,yb)\n",
    "            \n",
    "            # loss의 기울기 연산\n",
    "            loss.backward()\n",
    "            with torch.no_grad():\n",
    "                for p in model.parameters():\n",
    "                    p -= p.grad * lr\n",
    "                # 딥러닝에서 미니배치+loop 조합으로 parameter들을 업데이트하는데,\n",
    "                # 한 루프에서 업데이트를 위해 loss.backward()를 호출하면\n",
    "                # 각 파라미터들의 .grad 값에 기울기가 저장된다.\n",
    "                # 이러한 상태에서, zero_grad()를 하지 않고 back-propagation을 하면,\n",
    "                # 이전 loop에서 .grad에 저장된 값이 다음 loop의 업데이트에 간섭을 해서\n",
    "                # 원하는 방향을 학습이 되지 않게 된다.\n",
    "                # 따라서 루프가 한번 돌고나서 역전파를 하기 전에, 반드시 zero_grad()로\n",
    "                # .grad 값들을 0으로 초기화시킨 후 학습을 진행해야 한다.\n",
    "                model.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0814, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "print(loss_func(model(xb), yb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nn.Linear를 이용한 refactoring\n",
    "\n",
    "refactoring 전:\n",
    "class Mnist_Logistic(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.weights = nn.Parameter(torch.randn(784, 10) / math.sqrt(784))\n",
    "        self.bias = nn.Parameter(torch.zeros(10))\n",
    "\n",
    "    def forward(self, xb):\n",
    "        return xb @ self.weights + self.bias\n",
    "\n",
    "위와 같이, self.weights, self.bias를 수동으로 정의 및 초기화하고,\n",
    "xb @ self.weights + self.bias를 계산하지 않고 이 모든 것을 해주는\n",
    "PyTorch 클래스인 nn.Linear를 선형 layer로 사용한다.\n",
    "Pytorch 에는 다양한 유형의 코드를 크게 단순화 할 수 있는 미리 정의된 레이어가 있고 이는 또한 종종 기존 코드보다 속도를 빠르게 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nself.weights = nn.Parameter(torch.randn(784, 10) / math.sqrt(784))\\nself.bias = nn.Parameter(torch.zeros(10))\\nreturn xb @ self.weights + self.bias\\n\\n위와 아래가 같다.\\n\\nself.lin = nn.Linear(784,10)\\nreturn self.lin(xb)\\n'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# refactoring 후:\n",
    "class Mnist_Logistic(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.lin = nn.Linear(784,10)\n",
    "    \n",
    "    def forward(self,xb):\n",
    "        return self.lin(xb)\n",
    "\n",
    "\"\"\"\n",
    "self.weights = nn.Parameter(torch.randn(784, 10) / math.sqrt(784))\n",
    "self.bias = nn.Parameter(torch.zeros(10))\n",
    "return xb @ self.weights + self.bias\n",
    "\n",
    "위와 아래가 같다.\n",
    "\n",
    "self.lin = nn.Linear(784,10)\n",
    "return self.lin(xb)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.4036, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "model = Mnist_Logistic()\n",
    "print(loss_func(model(xb),yb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0815, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "train()\n",
    "print(loss_func(model(xb), yb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "최적화(optimization) 알고리즘을 가진 패키지인 torch.optim으로 refactoring 하기\n",
    "\n",
    "각 매개변수를 수동으로 업데이트 하는 대신, optimizer의 step 메서드를 사용해서\n",
    "업데이트를 진행할 수 있다.\n",
    "\n",
    "refactoring 전:\n",
    "with torch.no_grad():\n",
    "    for p in model.parameters(): p -= p.grad * lr\n",
    "    model.zero_grad()\n",
    "\n",
    "refactoring 후:\n",
    "opt.step()\n",
    "opt.zero_grad()\n",
    "\n",
    "opt.zero_grad()는 기울기를 0으로 재설정해준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "\n",
    "# 편의를 위해 함수 선언\n",
    "def get_model():\n",
    "    model = Mnist_Logistic()\n",
    "    return model, optim.SGD(model.parameters(),lr=lr)\n",
    "model,opt = get_model()\n",
    "\n",
    "def train():\n",
    "    for _ in range(epochs):\n",
    "        for i in range((n-1)//batch_size+1):\n",
    "            start_i = i * batch_size\n",
    "            end_i = start_i + batch_size\n",
    "            xb = x_train[start_i:end_i]\n",
    "            yb = y_train[start_i:end_i]\n",
    "            \n",
    "            pred = model(xb)\n",
    "            loss = loss_func(pred,yb)\n",
    "            \n",
    "            loss.backward()\n",
    "            with torch.no_grad():\n",
    "                opt.step()\n",
    "                opt.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0804, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "print(loss_func(model(xb), yb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset 을 이용하여 리팩토링하기\n",
    "PyTorch의 TensorDataset을 통해서 동일한 라인에서 독립변수와 종속변수에 쉽게 접근할 수 있다.\n",
    "\n",
    "refactoring 전:\n",
    "xb = x_train[start_i:end_i]\n",
    "yb = y_train[start_i:end_i]\n",
    "\n",
    "refactoring 후:\n",
    "train_ds = TensorDataset(x_train, y_train)\n",
    "xb,yb = train_ds[i*bs : i*bs+bs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataLoader를 이용하여 refactoring 하기\n",
    "\n",
    "PyTorch의 DataLoader는 배치 관리를 담당한다.\n",
    "DataLoader를 통해서 배치들에 대해서 반복하기 쉽게 된다.\n",
    "\n",
    "refactoring 전:\n",
    "for i in range((n-1)//bs + 1):\n",
    "    xb,yb = train_ds[i*bs : i*bs+bs]\n",
    "    pred = model(xb)\n",
    "\n",
    "refactoring 후:\n",
    "for xb,yb in train_dl:\n",
    "    pred = model(xb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 검증(validation) 추가하기\n",
    "overfitting을 확인하기 위해 항상 검증 데이터셋(validation set)이 있어야 한다.\n",
    "\n",
    "train dataset의 shuffling 이유:\n",
    "배치와 과적합 사이의 상관관계를 방지하기 위해서이다.\n",
    "\n",
    "validation loss는 validation dataset를 섞든 안섞든 동일하고,\n",
    "데이터를 섞는 것은 추가 시간이 걸리므로, 검증 데이터를 섞어서 이득이 없어서\n",
    "섞지 않는다.\n",
    "\n",
    "+ 검증 데이터셋에 대한 배치 크기는 학습 데이터셋의 배치 크기보다 크게 사용해도 좋다.\n",
    "이유:\n",
    "검증 데이터셋에 대해서는 역전파(backpropagation)이 필요하지 않으므로 메모리를 덜 사용할 수 있다.(기울기를 저장하기 위한 메모리가 필요가 없다.)\n",
    "따라서, 배치 크기를 키워서 손실을 더 빨리 계산한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train_ds = TensorDataset(x_train,y_train)\n",
    "train_dl = DataLoader(train_ds,batch_size=bs,shuffle=True)\n",
    "\n",
    "valid_ds = TensorDataset(x_valid,y_valid)\n",
    "valid_dl = DataLoader(valid_ds,batch_size=bs * 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor(0.3151)\n",
      "1 tensor(0.2806)\n",
      "tensor(0.0183, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "train_ds = TensorDataset(x_train,y_train)\n",
    "train_dl = DataLoader(train_ds,batch_size=batch_size,shuffle=True)\n",
    "\n",
    "valid_ds = TensorDataset(x_valid,y_valid)\n",
    "valid_dl = DataLoader(valid_ds,batch_size=batch_size * 2)\n",
    "\n",
    "model,opt = get_model()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # 훈련 전에 항상 model.train() 호출\n",
    "    model.train()\n",
    "    for xb,yb in train_dl:\n",
    "        pred = model(xb)\n",
    "        loss = loss_func(pred,yb)\n",
    "        \n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "    # 추론 전에 model.eval() 호출\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        valid_loss = sum(loss_func(model(xb),yb) for xb,yb in valid_dl)\n",
    "    print(epoch,valid_loss / len(valid_dl))\n",
    "print(loss_func(model(xb),yb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "훈련 데이터셋과 검증 데이터셋 모두에 대한 손실을 계산하는 프로세스를 2번 거치므로,이를 하나의 배치에 대한 손실을 계산하는 함수로 합치자.\n",
    "\n",
    "def loss_batch(model,loss_func,xb,yb,opt=None):\n",
    "    loss = loss_func(model(xb),yb)\n",
    "\n",
    "    # opt가 None == validation set -> 역전파를 수행하지 않는다.\n",
    "    if opt is not None:\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "    \n",
    "    return loss.item(), len(xb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_batch(model,loss_func,xb,yb,opt=None):\n",
    "    loss = loss_func(model(xb),yb)\n",
    "\n",
    "    # opt가 None == validation set -> 역전파를 수행하지 않는다.\n",
    "    if opt is not None:\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "    \n",
    "    return loss.item(), len(xb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def train(epochs,model,loss_func,opt,train_dl,valid_dl):\n",
    "    # epoch 횟수만큼 train을 반복함.\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        # train dataset에서 feature와 label을 나눠서 train loss 및 역전파 수행 및\n",
    "        # validation dataset loss도 계산\n",
    "        for xb,yb in train_dl:\n",
    "            loss_batch(model,loss_func,xb,yb,opt)\n",
    "        \n",
    "        model.eval()\n",
    "        # 역전파를 수행하지 않도록 torch.no_grad() context manager 사용\n",
    "        with torch.no_grad():\n",
    "            losses,nums = zip(\n",
    "                *[loss_batch(model,loss_func,xb,yb) for xb,yb in valid_dl]\n",
    "            )\n",
    "        val_loss = np.sum(np.multiply(losses,nums)) / np.sum(nums)\n",
    "        \n",
    "        print(epoch,val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(train_ds,valid_ds,batch_size):\n",
    "    return(\n",
    "        DataLoader(train_ds,batch_size=batch_size,shuffle=True),\n",
    "        DataLoader(valid_ds,batch_size=batch_size * 2)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.2949955606162548\n",
      "1 0.3541989100396633\n"
     ]
    }
   ],
   "source": [
    "# 모델 훈련\n",
    "train_dl,valid_dl = get_data(train_ds,valid_ds,batch_size)\n",
    "model,opt = get_model()\n",
    "\n",
    "\"\"\"\n",
    "def get_model():\n",
    "    model = Mnist_Logistic()\n",
    "    return model, optim.SGD(model.parameters(),lr=lr)\n",
    "model,opt = get_model()\n",
    "\"\"\"\n",
    "train(epochs,model,loss_func,opt,train_dl,valid_dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위에까지의 함수들 모두 '모델의 형식'에 대해 가정하지 않기 때문에\n",
    "별도의 수정없이 CNN을 학습하는 데 사용할 수 있다.\n",
    "\n",
    "CNN을 학습하기 위해 Conv2d 클래스를 컨볼루션 레이어로 사용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 선언\n",
    "class Mnist_CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1,16,kernel_size=3,stride=2,padding=1)\n",
    "        self.conv2 = nn.Conv2d(16,16,kernel_size=3,stride=2,padding=1)\n",
    "        self.conv3 = nn.Conv2d(16,10,kernel_size=3,stride=2,padding=1)\n",
    "    \n",
    "    def forward(self,xb):\n",
    "        # PyTorch의 view == Numpy의 reshpae\n",
    "        xb = xb.view(-1,1,28,28)\n",
    "        xb = F.relu(self.conv1(xb))\n",
    "        xb = F.relu(self.conv2(xb))\n",
    "        xb = F.relu(self.conv3(xb))\n",
    "        xb = F.avg_pool2d(xb,4)\n",
    "        return xb.view(-1,xb.size(1))\n",
    "lr = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.3448018862247467\n",
      "1 0.26320782202482224\n"
     ]
    }
   ],
   "source": [
    "# 모델 클래스 instanciate\n",
    "model = Mnist_CNN()\n",
    "# optimizer는 최소의 Cost로 결과를 찾아준다.\n",
    "# 모델을 생성할 때 조정하는 파라미터:\n",
    "# 에포크, 뉴런 수, 드롭아웃, 옵티마이저\n",
    "# optimizer: 학습 데이터셋을 이용하여 모델을 학습할 때 데이터의 실제 결과와\n",
    "# 모델이 예측한 결과를 기반을 잘 줄일 수 있게 만들어주는 역할이다.\n",
    "\n",
    "# 아래는 모멘텀(Momentum)을 이용하여, 이전 업데이트도 고려하고 일반적으로 더 빠른\n",
    "# 훈련으로 이어지는 확률적 경사하강법의 변형이다.\n",
    "opt = optim.SGD(model.parameters(),lr=lr,momentum=0.9)\n",
    "\n",
    "train(epochs,model,loss_func,opt,train_dl,valid_dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nn.Sequential\n",
    "Sequential 객체는 그 안에 포함된 각 모듈을 순차적으로 실행한다.\n",
    "이로 인해 우리의 신경망 코드를 더 간단하게 작성할 수 있다.\n",
    "\n",
    "Sequential을 활용하기 위해서는 custom layer를 쉽게 정의할 수 있어야 한다.\n",
    "아래 코드는 사용자 정의 레이어이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lambda(nn.Module):\n",
    "    def __init__(self,func):\n",
    "        super().__init__()\n",
    "        self.func = func\n",
    "    \n",
    "    def forward(self,x):\n",
    "        return self.func(x)\n",
    "    \n",
    "def preprocess(x):\n",
    "    return x.view(-1,1,28,28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.6501675142288208\n",
      "1 0.5299440441131592\n"
     ]
    }
   ],
   "source": [
    "model = nn.Sequential(\n",
    "    Lambda(preprocess),\n",
    "    nn.Conv2d(1,16,kernel_size=3,stride=2,padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(16,16,kernel_size=3,stride=2,padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(16,10,kernel_size=3,stride=2,padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.AvgPool2d(4),\n",
    "    Lambda(lambda x:x.view(x.size(0),-1))\n",
    ")\n",
    "\n",
    "opt = optim.SGD(model.parameters(),lr=lr,momentum=0.9)\n",
    "\n",
    "train(epochs,model,loss_func,opt,train_dl,valid_dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위에까지의 작성한 CNN은 MNIST에서만 작동한다.\n",
    "왜냐면 \n",
    "    1. 입력이 28*28의 긴 벡터라고 가정했기 때문이다.\n",
    "    2. 최종적으로 CNN 그리드 크기를 4*4로 가정했다.\n",
    "    (평균 풀링 커널 크기 때문)\n",
    "\n",
    "위의 2가지 가정을 제거해서 모델이 '모든' 2d 단일 채널(channel) 이미지에서\n",
    "작동하도록 해보자.\n",
    "초기의 Lambda 레이어를 제거하고 데이터 전처리를 제네레이터로 이동시킬 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(x,y):\n",
    "    return x.view(-1,1,28,28),y\n",
    "\n",
    "class WrappedDataLoader:\n",
    "    def __init__(self,dl,func):\n",
    "        self.dl = dl\n",
    "        self.func = func\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dl)\n",
    "    \n",
    "    def __iter__(self):\n",
    "        batches = iter(self.dl)\n",
    "        for b in batches:\n",
    "            yield(self.func(*b))\n",
    "\n",
    "train_dl, valid_dl = get_data(train_ds,valid_ds,batch_size)\n",
    "train_dl = WrappedDataLoader(train_dl,preprocess)\n",
    "valid_dl = WrappedDataLoader(valid_dl,preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nn.AvgPool2d 를 nn.AdaptiveAvgPool2d 로 대체하여 \n",
    "# 우리가 가진 입력 텐서가 아니라 원하는 출력 텐서의 크기를 정의할 수 있습니다. \n",
    "# 결과적으로 우리 모델은 모든 크기의 입력과 함께 작동합니다.\n",
    "nn.AvgPool2d: \n",
    "\n",
    "model = nn.Sequential(\n",
    "    Lambda(preprocess),\n",
    "    nn.Conv2d(1,16,kernel_size=3,stride=2,padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(16,16,kernel_size=3,stride=2,padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(16,10,kernel_size=3,stride=2,padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.AdaptiveAvgPool2d(1),\n",
    "    Lambda(lambda x:x.view(x.size(0),-1))\n",
    ")\n",
    "\n",
    "opt = optim.SGD(model.parameters(),lr=lr,momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.493562368285656\n",
      "1 0.4548331362128258\n"
     ]
    }
   ],
   "source": [
    "train(epochs, model, loss_func, opt, train_dl, valid_dl)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7559f6fd77b8427a5fb782dfa8ce0e49fdb43f868423430e09327111b43e761a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
